# Metrics Calculator
import time
from flask import Blueprint, request, jsonify
from typing import Dict, Any, Optional
import numpy as np
import torch
from werkzeug.utils import secure_filename
# backend/app.py
from pdfProcessing.pdfProcessor import PDFProcessor  # Note the lowercase 'p' in processor
from pdfProcessing.semanticSearch import SemanticScholar
from modelFolder.modelRunner import ModelInference
import os
# backend/app.py
from flask import Flask
from flask_cors import CORS
import os
import sys
import json
from pathlib import Path
from flask import Blueprint
from dotenv import load_dotenv
import os
from torch.nn.functional import cosine_similarity
from transformers import AutoTokenizer, AutoModel

class MetricsCalculator:
 


  def calculate_paper_comparison_metrics(self, seed_paper: Dict[str, Any], comparison_paper: Dict[str, Any], tokenizer, model) -> Dict[str, Any]:
        try:
            metrics = {
                'shared_author_count': 0,
                'shared_reference_count': 0,
                'shared_citation_count': 0,
                'reference_cosine': 0.0,
                'citation_cosine': 0.0,
                'abstract_cosine': 0.0
            }
            
            paper_info = comparison_paper['paper_info']
            seed_info = seed_paper['paper_info']
            
            # Get embedding only if abstract exists
            comp_abstract = paper_info.get('abstract', '')
            if comp_abstract:
                comp_embedding = self.get_scibert_embedding(comp_abstract, tokenizer, model)
                if comp_embedding is not None:
                    paper_info['scibert'] = comp_embedding.tolist()
            
            # Set operations for metrics
            seed_authors = set(seed_info.get('authors', []))
            comp_authors = set(paper_info.get('authors', []))
            metrics['shared_author_count'] = len(seed_authors & comp_authors)
            
            seed_refs = set(seed_info.get('references', []))
            comp_refs = set(paper_info.get('references', []))
            metrics['shared_reference_count'] = len(seed_refs & comp_refs)
            
            seed_cites = set(seed_info.get('citations', []))
            comp_cites = set(paper_info.get('citations', []))
            metrics['shared_citation_count'] = len(seed_cites & comp_cites)
            
            # Only calculate cosine similarities if needed
            if seed_refs and comp_refs:
                all_refs = list(seed_refs | comp_refs)
                seed_vec = torch.tensor([1 if ref in seed_refs else 0 for ref in all_refs], dtype=torch.float32)
                comp_vec = torch.tensor([1 if ref in comp_refs else 0 for ref in all_refs], dtype=torch.float32)
                metrics['reference_cosine'] = float(cosine_similarity(seed_vec.unsqueeze(0), comp_vec.unsqueeze(0)))
            
            if seed_cites and comp_cites:
                all_cites = list(seed_cites | comp_cites)
                seed_vec = torch.tensor([1 if cite in seed_cites else 0 for cite in all_cites], dtype=torch.float32)
                comp_vec = torch.tensor([1 if cite in comp_cites else 0 for cite in all_cites], dtype=torch.float32)
                metrics['citation_cosine'] = float(cosine_similarity(seed_vec.unsqueeze(0), comp_vec.unsqueeze(0)))
            
            # Calculate abstract similarity only if both exist
            if seed_info.get('abstract') and paper_info.get('abstract'):
                seed_words = seed_info['abstract'].lower().split()
                comp_words = paper_info['abstract'].lower().split()
                
                all_words = list(set(seed_words + comp_words))
                seed_vec = torch.tensor([seed_words.count(word) for word in all_words], dtype=torch.float32)
                comp_vec = torch.tensor([comp_words.count(word) for word in all_words], dtype=torch.float32)
                
                if torch.any(seed_vec) and torch.any(comp_vec):
                    metrics['abstract_cosine'] = float(cosine_similarity(seed_vec.unsqueeze(0), comp_vec.unsqueeze(0)))
            
            return metrics
            
        except Exception as e:
            print(f"Error calculating paper comparison metrics: {str(e)}")
            raise
        
  @torch.no_grad()  
  def get_scibert_embedding(self, text: str, tokenizer, model) -> Optional[torch.Tensor]:
        """Single text processing"""
        if not text or not isinstance(text, str) or not text.strip():
            return None
            
        inputs = tokenizer(
            text, 
            padding=True, 
            truncation=True, 
            max_length=512, 
            return_tensors="pt"
        )
        outputs = model(**inputs)
        return outputs.last_hidden_state[:, 0, :].squeeze()



  def get_relatively_similar_papers(self,papers):
    # Extract scores for analysis
    scores = [p['similarity_score'] for p in papers]
    
    # If we have less than 2 papers, just return them all
    if len(scores) < 2:
        return papers
        
    # Find natural breaks in the data
    sorted_papers = sorted(papers, key=lambda x: x['similarity_score'], reverse=True)
    sorted_scores = [p['similarity_score'] for p in sorted_papers]
    
    # Calculate gaps between consecutive scores
    gaps = [sorted_scores[i] - sorted_scores[i+1] for i in range(len(sorted_scores)-1)]
    
    if len(gaps) > 0:
        # Find the largest gap in the top 75% of papers
        consider_until = max(int(len(gaps) * 0.75), 1)  # Consider at least first gap
        largest_gap = max(gaps[:consider_until])
        gap_index = gaps.index(largest_gap)
        
        # If the gap is significant (more than 5% of the score range)
        score_range = max(scores) - min(scores)
        if largest_gap > score_range * 0.05:
            return sorted_papers[:gap_index + 1]
        else:
            # If no significant gap, use statistical approach
            mean = np.mean(scores)
            std = np.std(scores)
            return [p for p in papers if p['similarity_score'] > (mean + 0.25 * std)]
    
    return papers